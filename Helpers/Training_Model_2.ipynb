{"cells":[{"cell_type":"code","source":["#Copied from fastai notebook\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n#Importing data packages\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom mlflow import keras\nimport csv\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nnltk.download('wordnet')\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport tensorflow\nfrom collections import defaultdict\nimport re\nfrom bs4 import BeautifulSoup\nimport sys\nimport os\n\nos.environ['KERAS_BACKEND']='tensorflow' \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, AveragePooling1D, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.callbacks import CSVLogger, EarlyStopping\nfrom keras.callbacks import ModelCheckpoint"],"metadata":{"id":"UJMHHAXYf37_","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">%matplotlib inline is not supported in Databricks.\nYou can display matplotlib figures using display(). For an example, see https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html\nUsing TensorFlow backend.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils.vis_utils import plot_model"],"metadata":{"id":"NfMr9JnXjVEF","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["##Training Models"],"metadata":{"id":"yMHb8xc7fJLR","colab_type":"text"}},{"cell_type":"code","source":["def open_glove():\n  text = spark.read.text(\"/mnt/glove.6B.100d.txt\")\n  text = text.toPandas()\n  text_values = [str.split(str(item)) for item in text.iloc[:,0]]\n  return text_values"],"metadata":{"id":"Pr00hpZxfJLS","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def create_embedding(text_values):\n  embeddings_index = {}\n  for line in range(len(text_values)):\n      word = text_values[line][0]\n      coefs = np.asarray(text_values[line][1:], dtype='float32')\n      embeddings_index[word] = coefs\n\n  print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n  return embeddings_index"],"metadata":{"id":"izW0YP31fJLU","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def padsequence(vocab, sentences, maxwords, sequencelength):\n  tokenizer = Tokenizer(num_words=maxwords)  \n  tokenizer.fit_on_texts(vocab)\n  sequences = tokenizer.texts_to_sequences(sentences)\n  word_index = tokenizer.word_index\n  sequences = pad_sequences(sequences, padding='post', maxlen=sequencelength)\n  return tokenizer, sequences, word_index"],"metadata":{"id":"r7vG-19XfJLX","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def create_embedmatrix(word_index, embedding_index, embedding_dim):\n  embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n  for word, i in word_index.items():\n      embedding_vector = embedding_index.get(word)\n      if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n          embedding_matrix[i] = embedding_vector[:embedding_dim]\n  return embedding_matrix"],"metadata":{"id":"6jE_lp1AfJLZ","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["def create_embedlayer(word_index, embedding_index, embedding_dim, max_sequence):\n  embedding_matrix = create_embedmatrix(word_index, embedding_index, embedding_dim)\n  \n  embedding_layer = Embedding(len(word_index) + 1,\n                              embedding_dim,weights=[embedding_matrix],\n                              input_length=max_sequence,trainable=True)\n  nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n  print(\"Percentage of embedding matrix with weights: \" + (str(round(nonzero_elements / (len(word_index) + 1)*100))) + \"%\")\n  \n  return embedding_layer"],"metadata":{"id":"QqqzzvzWfJLe","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def splitdata(sequences, sequencelength, metadata, labels, val_split):\n  data = sequences\n  labels = to_categorical(np.asarray(labels))\n\n  indices = np.arange(data.shape[0])\n  np.random.shuffle(indices)\n  data = data[indices]\n  labels = labels[indices]\n  nb_validation_samples = int(val_split * data.shape[0])\n\n  x_train = data[:-nb_validation_samples]\n  y_train = labels[:-nb_validation_samples]\n  \n  x_val = data[-nb_validation_samples:]\n  y_val = labels[-nb_validation_samples:]\n\n  meta_train = metadata[:-nb_validation_samples]\n  meta_val = metadata[-nb_validation_samples:]\n\n  print('Shape of Text Data Tensor:', data.shape)\n  print('Shape of Metadata Tensor:', metadata.shape)\n  print('Shape of Label Tensor:', labels.shape)\n  \n  return x_train, y_train, x_val, y_val, meta_train, meta_val"],"metadata":{"id":"mGX_jFp4fJLh","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def choose_model(modeltype, embedding_layer, max_sequence, ):\n  if modeltype == \"ANN\":\n    model, cp, csv_logger, earlystop = create_ANN(embedding_layer, max_sequence)\n  elif modeltype == \"ANN_meta\":\n    model, cp, csv_logger, earlystop = create_ANN_meta(embedding_layer, max_sequence)\n  elif modeltype == \"CNN\":\n    model, cp, csv_logger, earlystop = create_CNN(embedding_layer, max_sequence)\n  elif modeltype == \"CNN_meta\":\n    model, cp, csv_logger, earlystop = create_CNN_meta(embedding_layer, max_sequence)\n  elif modeltype == \"LSTM\":\n    model, cp, csv_logger, earlystop = create_LSTM_Functional(embedding_layer, max_sequence)\n  elif modeltype == \"LSTM_meta\":\n    model, cp, csv_logger, earlystop = create_LSTM_meta(embedding_layer, max_sequence)\n  else:\n    print(\"No model found. Please change model type.\")\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{"id":"c7rG2Ka_fJLl","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["## ANN\ndef create_ANN(embedding_layer, max_sequence):\n  \n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  embedded_sequences = embedding_layer(sequence_input)\n  dense1 = Dense(128, activation='relu')(embedded_sequences)\n  l_flat = Flatten()(dense1) \n  dense2 = Dense(64, activation='relu')(l_flat)\n  preds = Dense(2, activation='softmax')(dense2)\n\n  model = Model(inputs=sequence_input, outputs=preds)\n  model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc'])\n\n  print(\"Simplified MLP\")\n  model.summary()\n  \n  #Create callbacks\n  cp=ModelCheckpoint('model_ann.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{"id":"fF3qCkxxfJLo","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["def create_CNN(embedding_layer, max_sequence):\n\n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  embedded_sequences = embedding_layer(sequence_input)\n  l_cov1= Conv1D(128, 10, activation='relu')(embedded_sequences)\n  l_pool1 = MaxPooling1D(5)(l_cov1)\n  l_cov2 = Conv1D(64, 5, activation='relu')(l_pool1)\n  l_pool2 = MaxPooling1D(5)(l_cov2)\n\n  l_flat = Flatten()(l_pool2)\n  l_dense = Dense(128, activation='relu')(l_flat)\n  preds = Dense(2, activation='softmax')(l_dense)\n\n  model = Model(sequence_input, preds)\n  model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc'])\n\n  print(\"Simplified convolutional neural network\")\n  model.summary()\n \n  #Create callbacks\n  cp=ModelCheckpoint('model_lstm.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{"id":"tUagHQ-dfJLs","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["#LSTM Functional\ndef create_LSTM(embedding_layer, max_sequence):\n  \n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  model = embedding_layer(sequence_input)\n  model = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(model)\n  model = Flatten()(model)\n  model = Dense(128, activation=\"relu\")(model)\n  model = Dropout(0.25)(model)\n  model = Dense(2, activation=\"softmax\")(model)\n\n  model = Model(inputs=sequence_input, outputs=model)\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  model.summary()\n\n \n  #Create callbacks\n  cp=ModelCheckpoint('model_lstm.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{"id":"SlcE0IHufJLw","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["from keras.callbacks import CSVLogger\nfrom keras import losses\nfrom keras.layers import concatenate\n\ndef create_LSTM_meta(embedding_layer, max_sequence):\n  \n  meta_input = Input(shape=(2,), name='meta_input')\n  y = Dense(32, activation='softmax')(meta_input)\n\n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  model = embedding_layer(sequence_input)\n  model = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(model)\n  model = Flatten()(model)\n  model = Dense(100, activation=\"relu\")(model)\n  model = Dropout(0.25)(model)\n\n  x = concatenate([model, y])\n  print(x)\n  l_dense = Dense(50, activation='relu')(x)\n  preds = Dense(2, activation='softmax')(l_dense)\n\n  model = Model(inputs=[meta_input,sequence_input], outputs=preds)\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n  model.summary()\n\n \n  #Create callbacks\n  cp=ModelCheckpoint('model_lstm.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["def load_data(complete_table, train_table):\n  #Load dataframes\n  complete_df = table_to_df(complete_table)\n  complete_df = prep_df(complete_df)\n\n  df_sentences_pos = table_to_df(train_table)\n  df_sentences_pos = prep_df(df_sentences_pos)\n\n  #Load glove file \n  text_values = open_glove()\n  #Create embedding_index from glove file\n  embedding_index = create_embedding(text_values)\n  return complete_df, df_sentences_pos, embedding_index"],"metadata":{"id":"RkcHQOecjVFJ","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["def make_model(modeltype, complete_df, df_sentences_pos, textcolumn_complete, textcolumn_train, labelcolumn_train, meta_train_start, meta_train_stop, max_sequence, max_words, embedding_dim, val_split, embedding_index, classweight_neg, classweight_pos, epoch, glove_emblayer):\n  if glove_emblayer == True:\n    #Vocabulary based on glove embedding\n    tokenizer, sequences, word_index = padsequence(embedding_index, df_sentences_pos.iloc[:,2], max_words, max_sequence)\n  else:\n    #Vocabulary based on corpus vocabulary\n    tokenizer, sequences, word_index = padsequence(complete_df.iloc[:,textcolumn_complete], df_sentences_pos.iloc[:,textcolumn_train], max_words, max_sequence)\n\n  #Make embedding layer\n  embedding_layer = create_embedlayer(word_index, embedding_index, embedding_dim, max_sequence)\n  \n  #Split data in train and validation set\n  x_train, y_train, x_val, y_val, meta_train, meta_val = splitdata(sequences, max_sequence, df_sentences_pos.iloc[:, meta_train_start:meta_train_stop], df_sentences_pos.iloc[:,labelcolumn_train], val_split)\n\n  #Create model and callbacks\n  model, cp, csv_logger, earlystop = choose_model(modeltype, embedding_layer, max_sequence)\n \n  #Define classweights\n  class_weight = {0: classweight_neg,\n                  1: classweight_pos}\n  \n  #Train model\n  history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs= epoch, batch_size=8, callbacks=[cp, csv_logger, earlystop], class_weight=class_weight)\n  \n  return model, history, cp, 'training.log'"],"metadata":{"id":"35_487b2fJL9","colab_type":"code","colab":{},"outputId":"98a491ca-c297-4653-d328-4236e576b240"},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def make_model_meta(modeltype, complete_df, df_sentences_pos, textcolumn_complete, textcolumn_train, labelcolumn_train, meta_train_start, meta_train_stop, max_sequence, max_words, embedding_dim, val_split, embedding_index, classweight_neg, classweight_pos, epoch, glove_emblayer):\n  if glove_emblayer == True:\n    #Vocabulary based on glove embedding\n    tokenizer, sequences, word_index = padsequence(embedding_index, df_sentences_pos.iloc[:,2], max_words, max_sequence)\n  else:\n    #Vocabulary based on corpus vocabulary\n    tokenizer, sequences, word_index = padsequence(complete_df.iloc[:,textcolumn_complete], df_sentences_pos.iloc[:,textcolumn_train], max_words, max_sequence)\n\n  #Make embedding layer\n  embedding_layer = create_embedlayer(word_index, embedding_index, embedding_dim, max_sequence)\n  \n  #Split data in train and validation set\n  x_train, y_train, x_val, y_val, meta_train, meta_val = splitdata(sequences, max_sequence, df_sentences_pos.iloc[:, meta_train_start:meta_train_stop], df_sentences_pos.iloc[:,labelcolumn_train], val_split)\n  meta_train = np.array(meta_train)\n  meta_val = np.array(meta_val)\n  #Create model and callbacks\n  model, cp, csv_logger, earlystop = choose_model(modeltype, embedding_layer, max_sequence)\n \n  #Define classweights\n  class_weight = {0: classweight_neg,\n                  1: classweight_pos}\n  #Train model\n  history=model.fit(x=[meta_train, x_train], y=y_train, validation_data=([meta_val, x_val], y_val),epochs= epoch, batch_size=8, callbacks=[cp, csv_logger, earlystop], class_weight=class_weight)\n  \n  return model, history, cp, 'training.log'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["from keras.callbacks import CSVLogger\nfrom keras import losses\nfrom keras.layers import concatenate\n\ndef create_CNN_meta(embedding_layer, max_sequence):\n\n\n  meta_input = Input(shape=(2,), name='meta_input')\n  y = Dense(32, activation='softmax')(meta_input)\n  \n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  embedded_sequences = embedding_layer(sequence_input)\n  l_cov1= Conv1D(128, 10, activation='relu')(embedded_sequences)\n  l_pool1 = MaxPooling1D(5)(l_cov1)\n  l_cov2 = Conv1D(64, 5, activation='relu')(l_pool1)\n  l_pool2 = MaxPooling1D(5)(l_cov2)\n  l_flat = Flatten()(l_pool2)\n  \n  x = concatenate([l_flat, y])\n  print(x)\n  l_dense = Dense(128, activation='relu')(x)\n  preds = Dense(2, activation='softmax')(l_dense)\n  \n  model = Model(inputs=[meta_input, sequence_input], outputs=preds)\n  model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc'])\n\n  print(\"Simplified convolutional neural network\")\n  model.summary()\n\n  #Create callbacks\n  cp=ModelCheckpoint('model_lstm.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["## ANN\ndef create_ANN_meta(embedding_layer, max_sequence):\n  from keras.callbacks import CSVLogger\n  from keras import losses\n  from keras.layers import concatenate\n  \n  meta_input = Input(shape=(2,), name='meta_input')\n  y = Dense(32, activation='softmax')(meta_input)\n\n  sequence_input = Input(shape=(max_sequence,), dtype='int32')\n  embedded_sequences = embedding_layer(sequence_input)\n  dense1 = Dense(128, activation='relu')(embedded_sequences)\n  l_flat = Flatten()(dense1) \n  dense2 = Dense(64, activation='relu')(l_flat)\n\n\n  x = concatenate([dense2, y])\n  print(x)\n  l_dense = Dense(128, activation='relu')(x)\n  preds = Dense(2, activation='softmax')(l_dense)\n\n  model = Model(inputs=[meta_input, sequence_input], outputs=preds)\n  model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['acc'])\n\n  print(\"Simplified MLP\")\n  model.summary()\n  \n  #Create callbacks\n  cp=ModelCheckpoint('model_ann.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n  csv_logger = CSVLogger('training.log')\n  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n  \n  return model, cp, csv_logger, earlystop"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"colab":{"name":"Training_Model (2).ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"name":"Training_Model_2","notebookId":164682937340107},"nbformat":4,"nbformat_minor":0}
