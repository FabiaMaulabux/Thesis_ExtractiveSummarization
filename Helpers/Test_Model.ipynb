{"cells":[{"cell_type":"markdown","source":["##Testing"],"metadata":{"colab_type":"text","id":"M5JwBq_nc_X4"}},{"cell_type":"code","source":["# import mlflow\n# from mlflow import keras\n# keras_model = mlflow.keras.load_model(\"ANN model\", run_id=\"af33541108df4983985c9f84160d0e22\")\n# model = keras_model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nWARNING:tensorflow:From /databricks/python/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nDeprecated in favor of operator or tf.math.divide.\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["#Importing data packages\nimport pandas as pd\nimport numpy as np\nimport mlflow\nfrom mlflow import keras\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nnltk.download('wordnet')\nimport pickle\nfrom collections import defaultdict\nimport re\nfrom bs4 import BeautifulSoup\nimport sys\nimport os\nos.environ['KERAS_BACKEND']='tensorflow' # Why theano why not\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["%run /Meetings/helpers/Data_Creation"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%run /Meetings/helpers/Training_Model_2"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def load_data2(df_original, df_new, start, stop):\n  \n  import pandas as pd\n  for i in range(start, stop):\n    transcript, summary = isolate(i, df_original)\n    transcript2 = [BOW(item) for item in transcript]\n    label = [Assign_Label(transcript, summary) for transcript in transcript]\n    score = Assign_Score(transcript2)\n    df_new = Append_DF(df_new, i + 1, transcript, label, score)\n  print(\"Completed loading \" + str(stop) + \" meetings to dataframe \" + str(df_new))\n  return df_new"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def load_test(summary_table, start, stop):\n  \n  ami_df = table_to_df(summary_table)\n  dftest = pd.DataFrame(columns=['Meeting ID', 'Transcript', 'Tokenized', 'Label','Length', 'Score'])\n  dftest = load_data2(ami_df, dftest, start, stop)\n  return dftest"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["def predictions_df(meeting, t, ground_truth, df):\n  \n  df = df.append({'Summary_ID': meeting + 1, 'Prediction': t, 'Ground Truth': ground_truth}, ignore_index=True)\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["def create_predictions(meeting, df, model, sentence_table, dftest, max_sequence, max_words, embedding_dim, summary_table, threshold):\n  dftest  = dftest[dftest['Meeting ID'] == meeting + 1]\n  dftest.iloc[:,3] = dftest.iloc[:,3].astype(str)\n\n  #Transform meeting for input in model\n  complete_df = table_to_df(sentence_table)\n  complete_df = prep_df(complete_df)\n  tokenizer, sequences, word_index = padsequence(complete_df.iloc[:,2], dftest.iloc[:,2], max_words, max_sequence)\n  y_test = dftest.iloc[:,3]\n  y_test = y_test.astype(str)\n\n  macronum=sorted(set(y_test))\n  macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n\n  def fun(i):\n      return macro_to_id[i]\n\n  y_test=y_test.apply(fun)\n\n  labels = y_test\n  \n  #predictions = keras_model.predict(data)\n  predictions = model.predict(sequences)\n  #Create transcript based on predictions and threshold\n  scores = ['none'] * len(predictions)\n  labels = [0] * len(predictions)\n  \n  for i in range(len(predictions)):\n    scores[i] = predictions[i][1]\n    if scores[i] > threshold:\n      labels[i] = 1\n  \n  dftest['Predicted'] = labels\n  transcript = dftest.Transcript[dftest.Predicted == 1]\n  \n  #Concatenate all transcript sentences\n  transcript = pd.DataFrame(transcript)\n  t = \"\"\n  \n  for i in range(len(transcript)):\n    t = t + str(transcript.iloc[i,0]) + \" \"\n  \n  #Grab original summary\n  ami_df = table_to_df(summary_table)\n  ground_truth = ami_df.iloc[meeting,3]\n  df = predictions_df(meeting, t, ground_truth, df)\n  return df\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def create_predictions_meta(meeting, df, model, sentence_table, dftest, max_sequence, max_words, embedding_dim, summary_table, threshold):\n  dftest  = dftest[dftest['Meeting ID'] == meeting + 1]\n  dftest.iloc[:,3] = dftest.iloc[:,3].astype(str)\n\n  #Transform meeting for input in model\n  complete_df = table_to_df(sentence_table)\n  complete_df = prep_df(complete_df)\n  tokenizer, sequences, word_index = padsequence(complete_df.iloc[:,2], dftest.iloc[:,2], max_words, max_sequence)\n  y_test = dftest.iloc[:,3]\n  y_test = y_test.astype(str)\n\n  macronum=sorted(set(y_test))\n  macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n\n  def fun(i):\n      return macro_to_id[i]\n\n  y_test=y_test.apply(fun)\n\n  labels = y_test\n  #predictions = keras_model.predict(data)\n  #meta_test = np.array(dftest.iloc[:,4:6], dtype=object)\n\n  #x_test = sequences.tolist()\n#   print(type(meta_test))\n#   print(type(sequences))\n  predictions = model.predict([dftest.iloc[:,4:6], sequences])\n  #Create transcript based on predictions and threshold\n  scores = ['none'] * len(predictions)\n  labels = [0] * len(predictions)\n  \n  for i in range(len(predictions)):\n    scores[i] = predictions[i][1]\n    if scores[i] > threshold:\n      labels[i] = 1\n  \n  dftest['Predicted'] = labels\n  transcript = dftest.Transcript[dftest.Predicted == 1]\n  \n  #Concatenate all transcript sentences\n  transcript = pd.DataFrame(transcript)\n  t = \"\"\n  \n  for i in range(len(transcript)):\n    t = t + str(transcript.iloc[i,0]) + \" \"\n  \n  #Grab original summary\n  ami_df = table_to_df(summary_table)\n  ground_truth = ami_df.iloc[meeting,3]\n  df = predictions_df(meeting, t, ground_truth, df)\n  return df\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["import rouge\nlog_array = [\"\"] * 4\n\ndef prepare_results(p, r, f, metric):\n    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n\n\ndef calculate_rouge(hypotheses, ground_truth):\n  log_array = [\"\"] * 4\n  i = 0 \n  for aggregator in ['Avg', 'Best', 'Individual']:\n    print('Evaluation with {}'.format(aggregator))\n    apply_avg = aggregator == 'Avg'\n    apply_best = aggregator == 'Best'\n\n    evaluator = rouge.Rouge(metrics=['rouge-n'\n                                      , 'rouge-l', 'rouge-w'\n                                    ],\n                           max_n=2,\n                           limit_length=False,\n                           length_limit_type='words',\n                           apply_avg=apply_avg,\n                           apply_best=apply_best,\n                           alpha=0.5, # Default F1_score\n                           weight_factor=1.2,\n                           stemming=True)\n\n    all_hypothesis = hypotheses\n    all_references = ground_truth\n\n    scores = evaluator.get_scores(all_hypothesis, all_references)\n\n    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n            for hypothesis_id, results_per_ref in enumerate(results):\n                nb_references = len(results_per_ref['p'])\n                for reference_id in range(nb_references):\n                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n                    print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n            print()\n        else:\n              print(prepare_results(results['p'], results['r'], results['f'], metric))\n              log_array[i] = prepare_results(results['p'], results['r'], results['f'], metric)\n              i += 1\n              \n    print()\n    return log_array"],"metadata":{"id":"LdFl6xVWap-d","colab_type":"code","colab":{}},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["def test_model(model, dftest, reload, summary_table, sentence_table, start, stop, max_sequence, max_words, embedding_dim, threshold):\n  if reload == True:\n    dftest = load_test(summary_table, start, stop)\n  \n  df_prediction = pd.DataFrame(columns=['Summary_ID', 'Prediction', 'Ground Truth'])\n\n  for i in range(start,stop):\n    df_prediction = create_predictions(i, df_prediction, model, sentence_table, dftest, max_sequence, max_words, embedding_dim, summary_table, threshold)\n  \n  hypotheses = df_prediction.iloc[:,1].values\n  ground_truth = df_prediction.iloc[:,2].values\n  \n  log_array = calculate_rouge(hypotheses, ground_truth)\n  \n  return dftest, df_prediction, log_array"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def test_model_meta(model, dftest, reload, summary_table, sentence_table, start, stop, max_sequence, max_words, embedding_dim, threshold):\n  if reload == True:\n    dftest = load_test(summary_table, start, stop)\n  \n  df_prediction = pd.DataFrame(columns=['Summary_ID', 'Prediction', 'Ground Truth'])\n\n  for i in range(start,stop):\n    df_prediction = create_predictions_meta(i, df_prediction, model, sentence_table, dftest, max_sequence, max_words, embedding_dim, summary_table, threshold)\n  \n  hypotheses = df_prediction.iloc[:,1].values\n  ground_truth = df_prediction.iloc[:,2].values\n  \n  log_array = calculate_rouge(hypotheses, ground_truth)\n  \n  return dftest, df_prediction, log_array"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"colab":{"name":"Test_Model.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"name":"Test_Model","notebookId":949061892807708},"nbformat":4,"nbformat_minor":0}
